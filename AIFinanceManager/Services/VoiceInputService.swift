//
//  VoiceInputService.swift
//  AIFinanceManager
//
//  Created on 2024
//

import Foundation
import Speech
import AVFoundation
import AVFAudio
import Combine

enum VoiceInputError: LocalizedError {
    case speechRecognitionNotAvailable
    case speechRecognitionDenied
    case speechRecognitionRestricted
    case audioEngineError(String)
    case recognitionError(String)
    
    var errorDescription: String? {
        switch self {
        case .speechRecognitionNotAvailable:
            return "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ –Ω–∞ —ç—Ç–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ"
        case .speechRecognitionDenied:
            return "–î–æ—Å—Ç—É–ø –∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é —Ä–µ—á–∏ –∑–∞–ø—Ä–µ—â–µ–Ω. –†–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –≤ –ù–∞—Å—Ç—Ä–æ–π–∫–∞—Ö"
        case .speechRecognitionRestricted:
            return "–î–æ—Å—Ç—É–ø –∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é —Ä–µ—á–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω"
        case .audioEngineError(let message):
            return "–û—à–∏–±–∫–∞ –∞—É–¥–∏–æ: \(message)"
        case .recognitionError(let message):
            return "–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: \(message)"
        }
    }
}

@MainActor
class VoiceInputService: NSObject, ObservableObject {
    @Published var isRecording = false
    @Published var transcribedText = ""
    @Published var errorMessage: String?

    private var audioEngine: AVAudioEngine?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let speechRecognizer: SFSpeechRecognizer?
    private var finalTranscription: String = ""
    private var isStopping: Bool = false // –§–ª–∞–≥ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤ stop

    // MARK: - Voice Activity Detection

    /// Silence detector for automatic stop
    private var silenceDetector: SilenceDetector?

    /// VAD enabled flag (can be toggled by user)
    @Published var isVADEnabled: Bool = VoiceInputConstants.vadEnabled

    // MARK: - Dynamic Context (iOS 17+)

    /// Weak references to ViewModels for contextual strings
    weak var categoriesViewModel: CategoriesViewModel?
    weak var accountsViewModel: AccountsViewModel?
    
    override init() {
        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        let recognizer = SFSpeechRecognizer(locale: Locale(identifier: "ru-RU"))
        recognizer?.delegate = nil // –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ—Å–ª–µ super.init()
        self.speechRecognizer = recognizer
        super.init()
        // –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º delegate –ø–æ—Å–ª–µ super.init()
        self.speechRecognizer?.delegate = self
    }
    
    // –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏
    var isSpeechRecognitionAvailable: Bool {
        guard let recognizer = speechRecognizer else { return false }
        return recognizer.isAvailable
    }
    
    // –ó–∞–ø—Ä–æ—Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π
    func requestAuthorization() async -> Bool {
        // –ó–∞–ø—Ä–æ—Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω (iOS 17+)
        let micStatus: Bool
        if #available(iOS 17.0, *) {
            micStatus = await AVAudioApplication.requestRecordPermission()
        } else {
            micStatus = await withCheckedContinuation { continuation in
                AVAudioSession.sharedInstance().requestRecordPermission { granted in
                    continuation.resume(returning: granted)
                }
            }
        }
        guard micStatus else {
            errorMessage = "–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–ø—Ä–µ—â–µ–Ω. –†–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –≤ –ù–∞—Å—Ç—Ä–æ–π–∫–∞—Ö"
            return false
        }
        
        // –ó–∞–ø—Ä–æ—Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
        let speechStatus = await withCheckedContinuation { continuation in
            SFSpeechRecognizer.requestAuthorization { status in
                continuation.resume(returning: status)
            }
        }
        
        switch speechStatus {
        case .authorized:
            return true
        case .denied:
            errorMessage = VoiceInputError.speechRecognitionDenied.errorDescription
            return false
        case .restricted:
            errorMessage = VoiceInputError.speechRecognitionRestricted.errorDescription
            return false
        case .notDetermined:
            errorMessage = "–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –Ω–µ –ø–æ–ª—É—á–µ–Ω–æ"
            return false
        @unknown default:
            errorMessage = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π"
            return false
        }
    }
    
    // –ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å
    func startRecording() async throws {
        #if DEBUG
        if VoiceInputConstants.enableParsingDebugLogs {
            print("\(VoiceInputConstants.debugLogPrefix) üé§ Starting recording...")
        }
        #endif

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
        guard let recognizer = speechRecognizer, recognizer.isAvailable else {
            #if DEBUG
            if VoiceInputConstants.enableParsingDebugLogs {
                print("\(VoiceInputConstants.debugLogPrefix) ‚ùå Speech recognizer not available")
            }
            #endif
            throw VoiceInputError.speechRecognitionNotAvailable
        }

        // –ï—Å–ª–∏ —É–∂–µ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º, –Ω–µ –¥–µ–ª–∞–µ–º –Ω–∏—á–µ–≥–æ
        if isRecording {
            #if DEBUG
            if VoiceInputConstants.enableParsingDebugLogs {
                print("\(VoiceInputConstants.debugLogPrefix) ‚ö†Ô∏è Already recording")
            }
            #endif
            return
        }
        
        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –∑–∞–ø–∏—Å—å, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
        await stopRecordingSync()
        
        // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        transcribedText = ""
        finalTranscription = ""
        errorMessage = nil
        isStopping = false

        // Initialize silence detector if VAD is enabled
        if isVADEnabled {
            silenceDetector = SilenceDetector()

            #if DEBUG
            if VoiceInputConstants.enableParsingDebugLogs {
                print("\(VoiceInputConstants.debugLogPrefix) VAD enabled - silence detector initialized")
            }
            #endif
        } else {
            silenceDetector = nil

            #if DEBUG
            if VoiceInputConstants.enableParsingDebugLogs {
                print("\(VoiceInputConstants.debugLogPrefix) VAD disabled")
            }
            #endif
        }
        
        // –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞—É–¥–∏–æ —Å–µ—Å—Å–∏—é
        let audioSession = AVAudioSession.sharedInstance()
        do {
            // –ò—Å–ø–æ–ª—å–∑—É–µ–º .playAndRecord –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            try audioSession.setCategory(.playAndRecord, mode: .measurement, options: [.duckOthers, .defaultToSpeaker])
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            throw VoiceInputError.audioEngineError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∞—É–¥–∏–æ —Å–µ—Å—Å–∏—é: \(error.localizedDescription)")
        }
        
        // –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            throw VoiceInputError.recognitionError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ")
        }
        
        // –ü–æ–∫–∞–∑—ã–≤–∞–µ–º partial results –¥–ª—è UI, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ final –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        recognitionRequest.shouldReportPartialResults = true
        
        // –£–ª—É—á—à–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ: –≤–∫–ª—é—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏
        if #available(iOS 13.0, *) {
            recognitionRequest.taskHint = .dictation
            // –í–∫–ª—é—á–∞–µ–º on-device recognition –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
            if recognizer.supportsOnDeviceRecognition {
                recognitionRequest.requiresOnDeviceRecognition = true
            }
        }

        // Dynamic Context Injection (iOS 17+)
        if #available(iOS 17.0, *) {
            let contextualStrings = buildContextualStrings()
            recognitionRequest.contextualStrings = contextualStrings

            #if DEBUG
            if VoiceInputConstants.enableParsingDebugLogs {
                print("\(VoiceInputConstants.debugLogPrefix) Added \(contextualStrings.count) contextual strings")
            }
            #endif
        }
        
        // –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞—É–¥–∏–æ engine
        audioEngine = AVAudioEngine()
        guard let audioEngine = audioEngine else {
            throw VoiceInputError.audioEngineError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∞—É–¥–∏–æ engine")
        }
        
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)

        inputNode.installTap(onBus: 0, bufferSize: VoiceInputConstants.audioBufferSize, format: recordingFormat) { [weak self] buffer, _ in
            // Send buffer to speech recognition
            recognitionRequest.append(buffer)

            // Analyze for silence detection if VAD is enabled
            if let self = self, self.isVADEnabled, let detector = self.silenceDetector {
                Task { @MainActor in
                    let silenceDetected = detector.analyzeSample(buffer)

                    if silenceDetected {
                        #if DEBUG
                        if VoiceInputConstants.enableParsingDebugLogs {
                            print("\(VoiceInputConstants.debugLogPrefix) üõë VAD triggered - stopping recording")
                        }
                        #endif

                        // Auto-stop recording
                        self.stopRecording()
                    }
                }
            }
        }
        
        // –ó–∞–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ engine
        audioEngine.prepare()
        do {
            try audioEngine.start()
        } catch {
            throw VoiceInputError.audioEngineError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –∞—É–¥–∏–æ engine: \(error.localizedDescription)")
        }
        
        // –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
        recognitionTask = recognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }

            if let result = result {
                Task { @MainActor in
                    let transcription = result.bestTranscription.formattedString
                    self.transcribedText = transcription

                    #if DEBUG
                    if VoiceInputConstants.enableParsingDebugLogs {
                        print("\(VoiceInputConstants.debugLogPrefix) Transcription: \(transcription)")
                        print("\(VoiceInputConstants.debugLogPrefix) isFinal: \(result.isFinal)")
                    }
                    #endif

                    // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
                    if result.isFinal {
                        self.finalTranscription = transcription
                    }
                }
            }

            if let error = error {
                #if DEBUG
                if VoiceInputConstants.enableParsingDebugLogs {
                    print("\(VoiceInputConstants.debugLogPrefix) Recognition error: \(error.localizedDescription)")
                }
                #endif

                Task { @MainActor in
                    if !self.isRecording {
                        // –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø–æ—Å–ª–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                        return
                    }
                    self.errorMessage = VoiceInputError.recognitionError(error.localizedDescription).errorDescription
                    self.stopRecording()
                }
            }
        }

        isRecording = true

        #if DEBUG
        if VoiceInputConstants.enableParsingDebugLogs {
            print("\(VoiceInputConstants.debugLogPrefix) ‚úÖ Recording started successfully")
        }
        #endif
    }
    
    // –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–ø–∏—Å—å (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è UI)
    func stopRecording() {
        Task { @MainActor in
            await stopRecordingSync()
        }
    }
    
    // –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–ø–∏—Å–∏
    // @MainActor –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç thread-safety, —Ç–∞–∫ –∫–∞–∫ –≤—Å–µ –≤—ã–∑–æ–≤—ã –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç –Ω–∞ –≥–ª–∞–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    private func stopRecordingSync() async {
        // –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤—ã–∑–æ–≤—ã
        guard !isStopping else { return }
        guard isRecording else { return }

        isStopping = true
        isRecording = false

        // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä–µ–∫—Ç—ã –ø–µ—Ä–µ–¥ –æ—á–∏—Å—Ç–∫–æ–π
        let currentAudioEngine = audioEngine
        let currentRecognitionRequest = recognitionRequest
        let currentRecognitionTask = recognitionTask

        // –ó–∞–≤–µ—Ä—à–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
        currentRecognitionRequest?.endAudio()

        // –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        try? await Task.sleep(nanoseconds: VoiceInputConstants.audioEngineStopDelayMs * 1_000_000)

        // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞—É–¥–∏–æ engine
        if let engine = currentAudioEngine, engine.isRunning {
            engine.stop()
            engine.inputNode.removeTap(onBus: 0)
        }
        audioEngine = nil

        recognitionRequest = nil

        // –û—Ç–º–µ–Ω—è–µ–º –∑–∞–¥–∞—á—É —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        currentRecognitionTask?.cancel()
        recognitionTask = nil

        // –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –∞—É–¥–∏–æ —Å–µ—Å—Å–∏—é
        do {
            try AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
        } catch {
            print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å–µ—Å—Å–∏–∏: \(error)")
        }

        // Reset silence detector
        silenceDetector?.reset()
        silenceDetector = nil

        // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        isStopping = false
    }
    
    // –ü–æ–ª—É—á–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    func getFinalText() -> String {
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞, –∏–Ω–∞—á–µ —Ç–µ–∫—É—â—É—é
        return finalTranscription.isEmpty ? transcribedText : finalTranscription
    }

    // MARK: - Dynamic Context Injection (iOS 17+)

    /// Build contextual strings for improved Speech Recognition
    /// - Returns: Array of contextual strings for better recognition of custom categories, accounts, etc.
    @available(iOS 17.0, *)
    private func buildContextualStrings() -> [String] {
        var context: [String] = []

        // 1. Account names with common patterns
        if let accountsVM = accountsViewModel {
            let accountNames = accountsVM.accounts.map { $0.name.lowercased() }
            context.append(contentsOf: accountNames)

            // Add variations: "–∫–∞—Ä—Ç–∞ X", "—Å—á–µ—Ç X", "—Å–æ —Å—á–µ—Ç–∞ X"
            for name in accountNames {
                context.append("–∫–∞—Ä—Ç–∞ \(name)")
                context.append("—Å—á–µ—Ç \(name)")
                context.append("—Å—á—ë—Ç \(name)")
                context.append("—Å –∫–∞—Ä—Ç—ã \(name)")
                context.append("—Å–æ —Å—á–µ—Ç–∞ \(name)")
                context.append("—Å–æ —Å—á—ë—Ç–∞ \(name)")
            }
        }

        // 2. Category names with common patterns
        if let categoriesVM = categoriesViewModel {
            let categoryNames = categoriesVM.customCategories.map { $0.name.lowercased() }
            context.append(contentsOf: categoryNames)

            // Add variations: "–Ω–∞ X", "–¥–ª—è X", "–≤ X"
            for name in categoryNames {
                context.append("–Ω–∞ \(name)")
                context.append("–¥–ª—è \(name)")
                context.append("–≤ \(name)")
            }
        }

        // 3. Subcategories
        if let categoriesVM = categoriesViewModel {
            let subcategoryNames = categoriesVM.subcategories.map { $0.name.lowercased() }
            context.append(contentsOf: subcategoryNames)
        }

        // 4. Common financial phrases
        let commonPhrases = [
            // Currencies
            "—Ç–µ–Ω–≥–µ", "—Ç–≥", "–¥–æ–ª–ª–∞—Ä", "–¥–æ–ª–ª–∞—Ä–æ–≤", "–µ–≤—Ä–æ", "—Ä—É–±–ª—å", "—Ä—É–±–ª–µ–π",
            // Transaction types
            "–ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ", "—Ä–∞—Å—Ö–æ–¥", "–¥–æ—Ö–æ–¥", "–ø–µ—Ä–µ–≤–æ–¥", "–æ–ø–ª–∞—Ç–∞", "–ø–æ–∫—É–ø–∫–∞",
            "–∑–∞—á–∏—Å–ª–µ–Ω–∏–µ", "—Å–ø–∏—Å–∞–Ω–∏–µ", "–≤–æ–∑–≤—Ä–∞—Ç",
            // Amount words
            "—Ç—ã—Å—è—á–∞", "—Ç—ã—Å—è—á", "–º–∏–ª–ª–∏–æ–Ω",
            // Time words
            "–≤—á–µ—Ä–∞", "—Å–µ–≥–æ–¥–Ω—è", "–ø–æ–∑–∞–≤—á–µ—Ä–∞"
        ]
        context.append(contentsOf: commonPhrases)

        // Remove duplicates and return
        return Array(Set(context))
    }
}

// MARK: - SFSpeechRecognizerDelegate
extension VoiceInputService: SFSpeechRecognizerDelegate {
    nonisolated func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        Task { @MainActor in
            if !available && isRecording {
                errorMessage = "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å—Ç–∞–ª–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"
                stopRecording()
            }
        }
    }
}
